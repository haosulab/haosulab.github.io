<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Policy Gradient</title>
    <meta name="description" content="" />
    <meta name="author" content="Hao Su" />
    <link rel="stylesheet" href="../extras/highlight/styles/github.css">
    <link rel="stylesheet" href="../extras/mermaid/mermaid.forest.css">
    <link href="../css/impress-common.css" rel="stylesheet" />
    <link href="css/classic-slides.css" rel="stylesheet" />
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/viz.js/1.7.1/viz.js"></script>
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/vs.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
    <style>


    mark.red {
    color:#ff0000;
    background: none;
}

    </style>
</head>
<body class="impress-not-supported">
    <div class="fallback-message">
        <p>Your browser <b>doesn't support the features required</b> by impress.js, so you are presented with a simplified version of this presentation.</p>
        <p>For the best experience please use the latest <b>Chrome</b>, <b>Safari</b> or <b>Firefox</b> browser.</p>
    </div>
    <div id="latex-macros"></div>
    <script src="./latex_macros.js"></script>
    <div id="impress"
         data-width="1920"
         data-height="1080"
         data-max-scale="3"
         data-min-scale="0"
         data-perspective="1000"
         data-transition-duration="0">
        <div class="step slide title" data-x="-2200" data-y="-3000" id="title">
            <h1 class="nt">Policy Gradient (Finite Action Space)</h1>
            <h2>Hao Su</h2>
            <h3>Spring, 2021</h3>
        </div>

        <div id="toc" class="step slide" data-rel-x="2200" data-rel-y="0">
            <h1 class="nt">Agenda</h1>
            <ul class="large">
                <li><a href="#motivation">Motivation</a></li>
                <li><a href="#pgt">The Policy Gradient Theorem</a></li>
                <li><a href="#est">Unbiasedly Estimate Policy Gradient</a></li>
                <li><a href="#better"> Practical First-Order Policy Optimization</a></li>
                <li><a href="#ppo"> Case Study: Proximal Policy Optimization (PPO)</a></li>
            </ul>
            click to jump to the section.
        </div>

        <!-- ################################################################### -->

        <div class="step slide separator" id="motivation">
            <h1 class="nt">Motivation</h1>
        </div>

        <!-- ################################################################### -->

        <div class="step slide">
            <h1 class="nt">First-Order Policy Optimization</h1>
            <ul>
                <li>Recall that a (step-independent) policy $\pi$ is just a function that maps from a state to a distribution over the action space.
                    <ul>
                        <li>The quality of $\pi$ is determined by $V^{\pi}(s_0)$, where $s_0$ is the initial state</li>
                        <li>We can parameterize $\pi$ by $\pi_{\theta}$, where $\theta\in\Theta$ and $\Theta$ is a parameter space.</li>
                    </ul>
                </li>
                <li>
                    Now we can formulate policy optimization as
                    \begin{align*}
                        \text{Maximize}_{\theta\in\Theta}V^{\pi_{\theta}}(s_0). 
                    \end{align*}
                </li>
            </ul>
            Can we use neural networks for $\Theta$ and do gradient descent? If only we knew how to calculate $\frac{\partial V^{\pi_{\theta}}(s_0)}{\partial \theta}$!
        </div>

        <!-- ################################################################### -->

        <div class="step slide separator" id="pgt">
            <h1 class="nt">The Policy Gradient Theorem</h1>
        </div>

        <!-- ################################################################### -->

        <div class="step slide">
            <h1 class="nt">PGT (Undiscounted) </h1>
            \begin{align*} 
                V^{\pi_{\theta}}(s) &= \sum_{a}\pi_{\theta}(s, a)\cdot Q^{\pi_{\theta}}(s, a)\\
                                       &=\sum_{a}\pi_{\theta}(s, a)\cdot\mathbb{E}_{s'\sim T(s, a)}\left[r(s, a) + V^{\pi_{\theta}}(s')\right].
            \end{align*}
            How to calculate $\nabla_{\theta}V^{\pi_{\theta}}(s_0)$? Note that,
            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}}(s_0)
                    &= \sum_{a}\nabla_{\theta}\left(\pi_{\theta}(s_0, a)\cdot\mathbb{E}_{s'\sim T(s, a)}\left[r(s, a) + V^{\pi_{\theta}}(s')\right]\right)\\
     \text{(product rule)} &= \sum_{a}
                            \left(
                                \nabla_{\theta}\pi_{\theta}(s_0, a) \cdot Q^{\pi_{\theta}}(s, a) 
                              + \pi_{\theta}(s_0, a)\cdot \mathbb{E}_{s'\sim T(s, a)}\left[\nabla_{\theta}V^{\pi_{\theta}}(s')\right]
                            \right)\\
     \text{(recursively repeat above)} &= \sum_s\sum_{k = 0}^{\infty}\mu_k(s)
                              \sum_{a}\nabla_{\theta}\pi_{\theta}(s, a) \cdot Q^{\pi_{\theta}}(s, a).
            \end{align*}
            Here $\mu_k(s)$ is the average visitation frequency of the state $s$ in step $k$.

            One question remains: $Q^{\pi_{\theta}}(s, a)$ is not directly available!
        </div>

        <div class="step slide">
            <h1 class="nt">PGT (Discounted) </h1>
            \begin{align*} 
                V^{\pi_{\theta}, \gamma}(s) &= \sum_{a}\pi_{\theta}(s, a)\cdot Q^{\pi_{\theta}, \gamma}(s, a)\\
                                       &=\sum_{a}\pi_{\theta}(s, a)\cdot\mathbb{E}_{s'\sim T(s, a)}\left[r(s, a) + \gamma\cdot V^{\pi_{\theta}, \gamma}(s')\right].
            \end{align*}
            How to calculate $\nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0)$? Note that,
            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0)
                    &= \sum_{a}\nabla_{\theta}\left(\pi_{\theta}(s_0, a)\cdot\mathbb{E}_{s'\sim T(s, a)}\left[r(s, a) + \gamma\cdot V^{\pi_{\theta}, \gamma}(s')\right]\right)\\
     \text{(product rule)} &= \sum_{a}
                            \left(
                                \nabla_{\theta}\pi_{\theta}(s_0, a) \cdot Q^{\pi_{\theta}, \gamma}(s, a) 
                              + \gamma\cdot\pi_{\theta}(s_0, a)\cdot \mathbb{E}_{s'\sim T(s, a)}\left[\nabla_{\theta}V^{\pi_{\theta}, \gamma}(s')\right]
                            \right)\\
     \text{(recursively repeat above)} &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)
                              \sum_{a}\nabla_{\theta}\pi_{\theta}(s, a) \cdot Q^{\pi_{\theta}, \gamma}(s, a).
            \end{align*}
            Here $\mu_k(s)$ is the average visitation frequency of the state $s$ in step $k$. Similarly to the episodic case, $Q^{\pi_{\theta}, \gamma}(s, a)$ is not directly available! <mark class="red">We will assume the discounted setting from now on</mark>.
        </div>

        <!-- ################################################################### -->

        <div class="step slide separator" id="est">
            <h1 class="nt">Unbiasedly Estimate Policy Gradient</h1>
        </div>

        <!-- ################################################################### -->

        <div class="step slide">
            <h1 class="nt">Creating an Unbiased Estimate for PG</h1>
            Let's say we have used $\pi_{\theta}$ to collect a rollout trajectory $\left\{(s_h, a_h, r_h)\right\}_{h = 0}^{\infty}$, where $s_h, a_h, r_h$ are random variables.

            Note that
            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0) 
                        &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)\sum_{a}\nabla_{\theta}\ln\left(\pi_{\theta}(s, a)\right) \cdot \pi_{\theta}(s, a) Q^{\pi_{\theta}, \gamma}(s, a)\\
                    &=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\sum_{a}\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a)\right)\cdot\pi_{\theta}(s_h, a)Q^{\pi_{\theta}, \gamma}(s_h, a)\right]\\
                    &=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h)\right)\cdot Q^{\pi_{\theta}, \gamma}(s_h, a_h)\right]\\
                   &=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h)\right)\cdot \sum_{i = h}^{\infty} \gamma^{i - h}\cdot r_i\right]\\
            \end{align*}
        </div>

        <div class="step slide">
            <h1 class="nt">Creating an Unbiased Estimate for PG (Cont'd)</h1>
            We have shown that
            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0)=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h)\right)\cdot \sum_{i = h}^{\infty} \gamma^{i - h}\cdot r_i\right]\\
            \end{align*}
            <ul>
                <li>Using more trajectories, we can get more accurate gradient estimate (smaller variance)</li>
                <li>Since the unbiased estimate is a summation, we can sample from the individual terms to do batched gradient descent</li>
            </ul>
        </div>

        <div class="step slide">
            <h1 class="nt">Advanced Value Estimates</h1>
            We have seen that we can use $\sum_{i = h}^{\infty} \gamma^{i - h} \cdot r_i$ as an unbiased estimate for $Q^{\pi_{\theta}, \gamma}(s_h, a_h)$.
            <br/>
            <br/>
            We can also have a value network $v_{\omega}(s)$ to try to memorize (estimates of) $V^{\pi_{\theta}, \gamma}(s)$ during the training. This way, whenever we need an estimate of $Q^{\pi_{\theta}, h}(s_h, a_h)$, we can use
                <ul>
                    <li>$e_{\infty} = \sum_{i = h}^{\infty} \gamma^{i - h}\cdot r_i$, which is unbiased but has high variance. </li>
                    <li>$e_h = r_h + \gamma\cdot v_{\omega}(s_{h + 1})$, which is biased but possibly has lower variance. </li>
                    <li>$e_k = \sum_{i = h}^k \gamma^{i - h}\cdot r_i + \gamma^{k - h + 1}\cdot v_{\omega}(s_{k + 1})$, which has a trade-off between the first two, depending on the choice of $k$. </li>
                    <li>$\sum_{i = h}^{\infty} \alpha_i e_i$, further combines different $e_i$'s with tunable weights $\alpha_i$'s that summing to $1$. </li>
                </ul>
        </div>

        <div class="step slide separator" id="better">
            <h1 class="nt">Practical First-Order Policy Optimization</h1>
        </div>

        <!-- ################################################################## -->

        <div class="step slide">
            <h1 class="nt">Advantage Estimates</h1>
            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0)
                    &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)
                              \left(\sum_{a}\nabla_{\theta}\pi_{\theta}(s, a) \cdot Q^{\pi_{\theta}, \gamma}(s, a) - 0\right)\\
                    &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)
                              \left(\sum_{a}\nabla_{\theta}\pi_{\theta}(s, a) \cdot Q^{\pi_{\theta}, \gamma}(s, a) - \sum_{a}\nabla_{\theta}\pi_{\theta}(s, a)\cdot V^{\pi_{\theta}, \gamma}(s)\right)\\
                    &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)
                              \sum_{a}\nabla_{\theta}\pi_{\theta}(s, a) \cdot \left(Q^{\pi_{\theta}, \gamma}(s, a) - V^{\pi_{\theta}, \gamma}(s)\right).\\
            \end{align*}
                $Q^{\pi_{\theta}, \gamma}(s, a) - V^{\pi_{\theta}, \gamma}(s)$ is called <mark class="red">advantage</mark>, which is typically denoted as $A^{\pi_{\theta}, \gamma}(s, a)$. In fact, the same derivation works if we replace $V^{\pi_{\theta}, \gamma}(s)$ by any quantity that depends only on $s$ (e.g., $0$, in our original derivation).
        </div>

        <div class="step slide">
            <h1 class="nt">Advantage Estimates (Cont'd)</h1>
            With the new representation of the policy gradient, we can now derive a new estimate of the policy gradient

            \begin{align*}
                \nabla_{\theta}V^{\pi_{\theta}, \gamma}(s_0) 
                        &= \sum_s\sum_{k = 0}^{\infty}\gamma^k\mu_k(s)\sum_{a}\nabla_{\theta}\ln\left(\pi_{\theta}(s, a)\right) \cdot \pi_{\theta}(s, a) A^{\pi_{\theta}, \gamma}(s, a)\\
                    &=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\sum_{a}\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a\right)\cdot\pi_{\theta}(s_h, a)A^{\pi_{\theta}, \gamma}(s_h, a)\right]\\
                    &=\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h\right)\cdot A^{\pi_{\theta}, \gamma}(s_h, a_h)\right]
            \end{align*}
            Without resampling, we cannot unbiasedly estimate the advantage, fortunately we can still use a value network.
        </div>

        <div class="step slide">
            <h1 class="nt">Advantage Estimates (Cont'd)</h1>
                Recall that we said $Q^{\pi_{\theta}, \gamma}(s_h, a_h)$ can be estiamted by $\sum_{i = h}^{\infty}\alpha_i e_i$ in general, where
            $$ \sum_{i = h}^{\infty} \alpha_i = 1$$
            and
                    $$e_k = \sum_{i = h}^k \gamma^{i - h}\cdot r_i + \gamma^{k - h + 1}\cdot v_{\omega}(s_{k + 1}).$$
            \begin{align*}
            \end{align*}
            
            The very popular General Advantage Estimate (GAE) estimates the advantage in the same fashion and it chooses $\alpha_i$ to be proportional to $\lambda^i$, where $\lambda\in[0, 1]$.
        </div>

        <div class="step slide">
            <h1 class="nt">Advantage Estimates (Cont'd)</h1>
               That is, the General Advantage Estimate (GAE) estimate $A^{\pi_{\theta}, \gamma}(s_h, a_h)$ 
            by
            \begin{align*}
                \hat{A}_{\text{GAE}(\lambda)}^{\pi_{\theta}, \gamma}(s_h, a_h) &= (1 - \lambda)\sum_{k = h}^{\infty} \lambda^{k - h}\left(\sum_{i = h}^k \gamma^{i - h}\cdot r_i + \gamma^{k - h + 1}\cdot v_{\omega}(s_{k + 1}) - v_{\omega}(s_{h})\right)\\
           (\text{calculation omitted}) &=\sum_{k = h}^{\infty}(\gamma\lambda)^{k - h} \left(r_k + \gamma v_{\omega}(s_{k + 1}) - v_{\omega}(s_{k})\right)
            \end{align*}
            Define $0^0 = 1$, we have
            \begin{align*}
                \hat{A}_{\text{GAE}(0)}^{\pi_{\theta}, \gamma}(s_h, a_h) = r_h + \gamma v_{\omega}(s_{h + 1}) - v_{\omega}(s_{h}).
            \end{align*}
            We also have
            \begin{align*}
                \hat{A}_{\text{GAE}(1)}^{\pi_{\theta}, \gamma}(s_h, a_h) = \sum_{i = h}^{\infty}\gamma^{i - h} r_i - v_{\omega}(s_h).
            \end{align*}
            
        </div>

        <div class="step slide">
            <h1 class="nt">Some Little Detail</h1>
            Given any advantage estimate $\hat{A}^{\pi_{\theta}, \gamma}(s_h, a_h)$, we can estimate the policy gradient by
            \begin{align*}
                \hat{\nabla}_{\theta}V^{\pi_{\theta}, \gamma}(s_0) 
                   =\mathbb{E}\left[\sum_{h = 0}^{\infty}\gamma^h\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h)\right)\cdot \hat{A}^{\pi_{\theta}, \gamma}(s_h, a_h)\right].
            \end{align*}
           However, in most implementations, people simply use
            \begin{align*}
                \hat{\nabla}_{\theta}V^{\pi_{\theta}, \gamma}(s_0) 
                   =\mathbb{E}\left[\sum_{h = 0}^{\infty}\nabla_{\theta}\ln\left(\pi_{\theta}(s_h, a_h)\right)\cdot \hat{A}^{\pi_{\theta}, \gamma}(s_h, a_h)\right].
            \end{align*}
        </div>

        <div class="step slide">
            <h1 class="nt">Escaping Local optima</h1>
            Because policy gradient is essentially first-order optimization of a non-convex function, no convergence is guaranteed. People use regularization (exploration term) to encourage escaping local optima. For example, instead of taking the gradient of 
            \begin{align*}
                \theta' \mapsto \mathbb{E}\left[\sum_{h = 0}^{\infty}\ln\left(\pi_{\theta'}(s_h, a_h)\right)\cdot \hat{A}^{\pi_{\theta}, \gamma}(s_h, a_h)\right]
            \end{align*}
            at $\theta$, people instead take the gradient of
            \begin{align*}
                \theta'\mapsto\mathbb{E}\left[\sum_{h = 0}^{\infty}\ln\left(\pi_{\theta'}(s_h, a_h)\right)\cdot \hat{A}^{\pi_{\theta}, \gamma}(s_h, a_h) + \eta \cdot\,\text{entropy}(\pi_{\theta'}(s_h, \cdot))\right]
            \end{align*}
            at $\theta$.
        </div>


        <div class="step slide">
            <h1 class="nt">More Stable Policy Optimization</h1>
            Now that we know how to estimate policy gradients, any method/trick that can be applied to general first-order optimization can in principle to be used for policy optimization. 
           <br /> 
           <br /> 
            Naive gradient descent does not work very well in practice: rollouts are expensive, so we typically do multiple gradient descents on the same set of rollouts, these descents will become more and more biased. Some remedies:
            <ul>
                <li>TRPO (Trust Region Policy Optimization): encourages the updated policy to be close to the previous policy in terms of $\text{KL}(\pi_{\text{new}}(s, \cdot)||\pi_{\text{old}}(s, \cdot))$, w.r.t. some distribution over $s$.</li>
                <li>PPO (Proximal Policy Optimization): encourages the updated policy to be close to the previous policy in terms of $\left|\frac{\pi_{\text{new}}(s, \cdot)}{\pi_{\text{old}}(s, \cdot))} - 1\right|$, w.r.t. some distribution over $s$.</li>
            </ul>
        </div>

        <div class="step slide separator" id="ppo">
            <h1 class="nt">Case Study: Proximal Policy Optimization (PPO)</h1>
        </div>

        <!-- ################################################################## -->

        <div class="step slide">
            <h1 class="nt">PPO</h1>
            PPO repeats the following procedure:
            <ul>
                <li>Sample multiple (say, 128) trajectories of certain length (say, 128) to get a replay buffer of state-actions pairs (say, 128 * 128 $(s, a)$ pairs)</li>
                <li>Estimate the advantage of each $(s, a)$ pair using GAE (say GAE($0.95$)) and the corresponding trajectory</li>
                <li> Do (mini-batch) gradient descent w.r.t to an objective function multiple times (say, mini-batch of size 128 * 32, 16 gradient descents)</li>
            </ul>
        </div>

        <div class="step slide">
            <h1 class="nt">PPO - Objective Function</h1>
            Suppose the current replay buffer is generated using $\pi_{\theta}$, the regular (non-proximal) gradient update uses the following objective function at $\theta$ (note the gradient of the $\ln(\pi_{\theta'}(s, a))$ is expanded):
            
            \begin{align*}
                &\theta'\mapsto\mathbb{E}_{(s, a)\sim\text{replay buffer}}\\
            &\ \ \ \ \left[\frac{\pi_{\theta'}(s, a)}{\pi_{\theta}(s, a)}\cdot \hat{A}^{\pi_{\theta}, \gamma}(s, a) + \eta \cdot\,\text{entropy}(\pi_{\theta'}(s, \cdot))\right].
            \end{align*}
            
            
            PPO use the following objective function for gradient descents at $\theta$:
            \begin{align*}
                &\theta'\mapsto\mathbb{E}_{(s, a)\sim\text{replay buffer}}\\
            &\ \ \ \ \left[\text{min}\left(\frac{\pi_{\theta'}(s, a)}{\pi_{\theta}(s, a)}\cdot \hat{A}^{\pi_{\theta}, \gamma}(s, a), \text{clip}\left(\frac{\pi_{\theta'}(s, a)}{\pi_{\theta}(s, a)}, 1 - \epsilon, 1 + \epsilon\right)\cdot \hat{A}^{\pi_{\theta}, \gamma}(s, a)\right) + \eta \cdot\,\text{entropy}(\pi_{\theta'}(s, \cdot))\right].
            \end{align*}
        </div>


    <div id="overview" class="step" data-x="4500" data-y="1500" data-scale="10" style="pointer-events: none;"></div>
    </div>
    <!--
        Add navigation-ui controls: back, forward and a select list.
        Add a progress indicator bar (current step / all steps)
        Add the help popup plugin
    -->
    <div id="impress-toolbar"></div>

    <div class="impress-progressbar"><div></div></div>
    <div class="impress-progress"></div>

    <div id="impress-help"></div>

    <script type="text/javascript" src="../extras/highlight/highlight.pack.js"></script>
    <script type="text/javascript" src="../extras/mermaid/mermaid.min.js"></script>
    <script type="text/javascript" src="../extras/markdown/markdown.js"></script>
    <!--
        To make all described above really work, you need to include impress.js in the page.
        You also need to call a `impress().init()` function to initialize impress.js presentation.
        And you should do it in the end of your document.
    -->
    <script type="text/javascript" src="../js/impress.js">
    </script>
    <script type="text/javascript">
        (function () {
            var vizPrefix = "language-viz-";
            Array.prototype.forEach.call(document.querySelectorAll("[class^=" + vizPrefix + "]"), function (x) {
                var engine;
                x.getAttribute("class").split(" ").forEach(function (cls) {
                    if (cls.startsWith(vizPrefix)) {
                        engine = cls.substr(vizPrefix.length);
                    }
                });
                var image = new DOMParser().parseFromString(Viz(x.innerText, { format: "svg", engine: engine }), "image/svg+xml");
                x.parentNode.insertBefore(image.documentElement, x);
                x.style.display = 'none'
                x.parentNode.style.backgroundColor = "white"
            });
        })();
    </script>
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js", "color.js"],
                },
                jax: ["input/TeX", "output/SVG"]
            },
            AuthorInit: function () {
                MathJax.Hub.Register.StartupHook("Begin", function () {
                    MathJax.Hub.Queue(function () {
                        var all = MathJax.Hub.getAllJax(), i;
                        for (i = 0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                        }
                    })
                });
            }
        };
    </script>
    <script type="text/javascript" src="../extras/mathjax/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>impress().init();</script>
    <!--
            <script>
                function setSlideID() {
        x = document.getElementsByClassName("slide");
        const titleSet = new Set();
        for (var i = 0; i < x.length; i++) {
            h1 = x[i].getElementsByTagName("h1")[0];
                // alert(title);
            title = h1.innerHTML.replace(/\W/g, '_');
            if (titleSet.has(title)) {
                continue;
            }
            x[i].id = title;
            titleSet.add(title);
        }
    }
            </script>
            <script> setSlideID(); </script>
            -->
</body>
</html>
<!-- discarded -->
