<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>L13</title>
        <meta name="description" content="" />
        <meta name="author" content="Hao Su" />
        <link rel="stylesheet" href="../extras/highlight/styles/github.css">
        <!--
            -<link rel="stylesheet" href="../extras/mermaid/mermaid.forest.css">
        -->
        <link rel="stylesheet" href="../extras/mermaid/mermaid.dark.css">
        <link href="../css/impress-common.css" rel="stylesheet" />   
        <link href="css/classic-slides.css" rel="stylesheet" />
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/viz.js/1.7.1/viz.js"> </script>
        <link rel="stylesheet"
              href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/vs.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
    </head>
    <body class="impress-not-supported">
        <div class="fallback-message">
            <p>Your browser <b>doesn't support the features required</b> by impress.js, so you are presented with a simplified version of this presentation.</p>
            <p>For the best experience please use the latest <b>Chrome</b>, <b>Safari</b> or <b>Firefox</b> browser.</p>
        </div>
        <div id="latex-macros"></div>
        <script src="./latex_macros.js"></script>
        <div id="impress"
             data-width="1920"
             data-height="1080"
             data-max-scale="3"
             data-min-scale="0"
             data-perspective="1000"
             data-transition-duration="0"
             >
             <div id="toc" class="step slide" data-rel-x="2200" data-rel-y="0">
                 <h1 class="nt">Mid-Term Course Survey</h1>
                 <ul>
                     <li>18 students participated so far. As promised, each will receive 1 extra credit.</li>
                     <li>Selected suggestions:
                         <ul>
                             <li>Maybe have a pdf version of the slide before the lecture ...</li>
                             <li>Some provided suggestions on tuning the parameters in some of the homework ...</li>
                             <li>For some of the long math derivations, it would be helpful to have some sort of visual aid of what is being derived ...</li>
                             <li>Provide more demos to sapien. </li>
                             <li>more guest lecture (professor or senior student) if it is possible</li>
                             <li>This is probably too novel, but I think a "flipped" instruction mode would do this class wonder.</li>
                             <li>It would better to cover some material for collision checking in the future</li>
                             <li>...some of the material, such as the initial discussion of the three frames, it would take some time offline to truly appreciate the material...</li>
                         </ul>
                     </li>
                 </ul>
             </div>
             <div class="step slide title" data-x="-2200" data-y="-3000" id="title">
                 <h1 class="nt">L13: Framework of Reinforcement Learning (II)</h1>
                 <h2>Hao Su
                     <p style="font-size:30px">(slides prepared with the help from Tongzhou Mu)</p>
                 </h2>
                 <h3>Spring, 2021</h3>
                 <div class="ack">Contents are based on <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a> from Prof. Richard S. Sutton and Prof. Andrew G. Barto, and <a href="https://www.davidsilver.uk/teaching/">COMPM050/COMPGI13</a> taught at UCL by Prof. David Silver.</div>
             </div>

             <div id="toc" class="step slide" data-rel-x="2200" data-rel-y="0">
                 <h1 class="nt">Agenda</h1>
                 <ul class="large" id="agenda"></ul>
                 click to jump to the section.
             </div>

             <!-- ################################################################### -->
             <!-- # New section                                                     # -->
             <!-- ################################################################### -->
             <div class="step slide separator" id="optimal">
                 <h1 class="nt">Optimal Policy and Optimal Value Function</h1>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="vt">Optimal Value Function</h1>
                 <ul>
                     <li>Due to the Markovian property, the return starting from a state $s$ is independent of its history. Therefore, we can compare the return of all policies starting from $s$ and find the optimal one.</li>
                     <li>The optimal state-value function $V^*(s)$ is the maximum value function over all policies
                         <ul>
                             <li>$V^*(s)=\max_\pi V_\pi(s)$</li>
                         </ul>
                     </li>
                     <li>The optimal action-value function $Q_*(s,a)$ is the maximum action-value function over all policies
                         <ul>
                             <li>$Q^*(s,a)=\max_\pi Q_\pi(s,a)$</li>
                         </ul>
                     </li>
                     <li>The optimal value function specifies the best possible performance in the MDP.</li>
                 </ul>
             </div>
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Optimal Policy</h1>
                 <ul>
                     <li>Define a partial ordering over policies
                         \[
                         \pi\ge\pi'\mbox{ if }V_\pi(s)\ge V_{\pi'}(s), \forall s
                         \]
                     </li>
                     <blockquote>
                         Theorem: For any Markov Decision Process
                         <ul>
                             <li>There exists an optimal policy $\pi_*$ that is better than, or equal to, all other policies, $\pi_*\ge\pi,~\forall\pi$</li>
                             <li>All optimal policies achieve the optimal value function, $V_{\pi^*}(s)=V^*(s)$</li>
                             <li>All optimal policies achieve the optimal action-value function, $Q_{\pi^*}(s,a)=Q^*(s,a)$</li>
                         </ul>
                     </blockquote>
                     <!-- <li>An optimal policy can be found by maximising over $q_*(s,a)$,</li> -->
                     <li>An optimal policy can be found by maximizing over $Q^*(s,a)$,
                         \[
                         \pi^*(a|s)=
                         \begin{cases}
                         1, & \text{if}~~a=\text{argmax}_{a\in\mc{A}}~Q^*(s,a) \\  
                         0, & \text{otherwise}   
                         \end{cases}
                         \]
                     </li>

                 </ul>
             </div>
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Bellman Optimality Equation</h1>
                 <ul>
                     <li>Optimal value functions also satisfy recursive relationships
                         \[
                         \begin{aligned}
                         V^*(s)
                         & = \max_a\bb{E}_{\pi_*}[G_t|S_t=s, A_t=a] \\
                         & = \max_a\bb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a] \\
                         & = \max_a\bb{E}[R_{t+1}+\gamma V^*(S_{t+1})|S_t=s, A_t=a] \\
                         \end{aligned}
                         \]
                     </li>
                     <li>Similarly, for action-value function, we have</li>
                     \[
                     Q^*(s,a)=\bb{E}[R_{t+1}+\gamma \max_{a'}Q^*(S_{t+1}, a')|S_t=s, A_t=a]
                     \]
                     <li>They are called <b>Bellman Optimality Equations</b></li>
                 </ul>
             </div>
             <!--
                 -[> ################################################################### <]
                 -<div class="step slide">
                 -    <h1 class="nt">Visualizations of Bellman Optimality Equations</h1>
                 -    <div class="row">
                 -        <div class="column" style="flex: 20%">
                 -            <div style=margin-top:10px>
                 -                <img src="./L12/bellman_opt_v.png" width="85%"/>
                 -            </div>
                 -        </div>
                 -        <div class="column" style="flex: 20%">
                 -            <div style=margin-top:10px>
                 -                <img src="./L12/bellman_opt_q.png" width="100%"/>
                 -            </div>
                 -        </div>
                 -    </div>
                 -</div>
             -->
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="vt">Solving the Bellman Optimality Equation</h1>
                 <ul>
                     <li>Bellman Optimality Equation is non-linear (because there is the max operation).</li>
                     <li>No closed form solution (in general)</li>
                     <li>Many iterative solution methods:
                         <ul>
                             <li>Value Iteration</li>
                             <li>Policy Iteration</li>
                             <li>Q-learning (we will talk about this later)</li>
                             <li>SARSA</li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <!-- # New section                                                     # -->
             <!-- ################################################################### -->
             <div class="step slide separator" id="estimate_value">
                 <h1 class="nt">Estimating Value Function for a Given Policy</h1>
             </div>

             <!--
                 -[> ################################################################### <]
                 -<div class="step slide">
                 -    <h1 class="nt">Visualizations of Bellman Expectation Equations</h1>
                 -    <div class="row">
                 -        <div class="column" style="flex: 20%">
                 -            <div style=margin-top:10px>
                 -                <img src="./L12/bellman_v.png" width="85%"/>
                 -            </div>
                 -        </div>
                 -        <div class="column" style="flex: 20%">
                 -            <div style=margin-top:10px>
                 -                <img src="./L12/bellman_q.png" width="100%"/>
                 -            </div>
                 -        </div>
                 -    </div>
                 -</div>
             -->


             <!-- ################################################################### -->
             <div class="step slide">
                 <div style="margin-top: 400px"></div>
                 <center>
                     <div class="large"><b>Goal</b>: Given a policy $\pi(a|s)$, estimate the value of the policy.</div>
                 </center>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Monte-Carlo Policy Evaluation</h1>
                 <ul>
                     <li>Basic idea: MC uses the simplest possible idea: <b>value = mean return</b></li>
                 </ul>
                 <ul class="substep">
                     <li>Learn $V_\pi$ from $K$ episodes under policy $\pi$
                         <ul>
                             <li>$\{S_{k,0}, A_{k,0}, R_{k,1}, ..., S_{k,T}\}_{k=1}^K\sim\pi$</li>
                         </ul>
                     </li>
                     <li>Recall that the <i>return</i> is the total discounted reward:
                         <ul>
                             <li>$G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-t-1}R_T$</li>
                         </ul>
                     </li>
                     <li>Recall that the value function is the expected return:
                         <ul>
                             <li>$V_\pi(s)=\bb{E}_\pi[G_t|S_t=s]$</li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Monte-Carlo Policy Evaluation</h1>
                 <ul>
                     <li>Suppose that we have collected a number of trajectories. Monte-Carlo policy evaluation uses <i>empirical mean return</i> to approximate <i>expected return</i>
                         <ul>
                             <li>For each episode:
                                 <ul>
                                     <li>For each time step $t$:
                                         <ul>
                                             <li>Compute empirical return $G_t$ from the current state $s$</li>
                                             <li>Increment total return $S(s) \leftarrow S(s) + G_t$</li>
                                             <li>Increment state visit counter $N(s) \leftarrow N(s) + 1$</li>
                                         </ul>
                                     </li>
                                 </ul>
                                 <li>Value is estimated by mean return $V(s)=S(s)/N(s)$</li>
                         </ul>
                             </li>
                 </ul>
             </div>

             <!--
                 -[> ################################################################### <]
                 -<div class="step slide">
                 -    <h1 class="nt">Incremental Monte-Carlo Updates</h1>
                 -    <ul>
                 -        <li>We can also update $V(s)$ incrementally after episode $S_0, A_0, R_1, ..., S_T$.</li>
                 -        <li class="substep">Consider a general problem of computing the mean for stream data
                 -            <ul>
                 -                <li>The mean $\mu_1, \mu_2, ...$ of a sequence of general vectors $x_1, x_2, ...$ can be computed incrementally,</li>
                 -                <li>$\mu_k=\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})$</li>
                 -            </ul>
                 -        </li>
                 -        <li class="substep">For each state $S_t$ with return $G_t$
                 -            <ul>
                 -                <li>$N(S_t) \leftarrow N(S_t) + 1$</li>
                 -                <li>$V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t-V(S_t))$</li>
                 -                <li>It can be proved that $V(s)\to V_{\pi}(s)$.</li>
                 -            </ul>
                 -        </li>
                 -        <li class="substep">In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.
                 -            <ul>
                 -                <li>$V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))$</li>
                 -                <li>You may regard the $\alpha$ as the learning rate in supervised learning</li>
                 -                <li>May <b>not</b> converge for stationary problems. For small $\alpha$, good enough.</li>
                 -            </ul>
                 -        </li>
                 -    </ul>
                 -</div>
             -->

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Monte-Carlo Methods</h1>
                 <ul>
                     <li>Quick facts:
                         <ul>
                             <li>MC is unbiased (average of the empirical return is the true return)</li>
                             <li>MC methods learn directly from episodes of experience</li>
                             <li>MC is model-free: no knowledge of MDP transitions / rewards</li>
                         </ul>
                     </li>
                     <li>Caveat: can only apply MC to episodic MDPs
                         <ul>
                             <li>All episodes must terminate</li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="vt">Temporal-Difference Learning</h1>
                 <ul>
                     <li>Basic idea: TD leverages <b>Bellman expectation equation</b> to update the value function.
                         \[
                         V_{\pi}(s)= \bb{E}_{\pi}[R_{t+1}+\gamma V_{\pi}(S_{t+1})|S_t=s]
                         \]
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="et">Temporal-Difference Learning</h1>
                 \[
                 V_{\pi}(s)= \bb{E}_{\pi}[R_{t+1}+\gamma V_{\pi}(S_{t+1})|S_t=s]
                 \]
                 <ul>
                     <li>Learn $V_\pi$ from $K$ episodes under policy $\pi$
                         <ul>
                             <li>$\{S_{k,0}, A_{k,0}, R_{k,1}, ..., S_{k,T}\}_{k=1}^K\sim\pi$</li>
                         </ul>
                     </li>
                     <li>Simplest temporal-difference learning algorithm: TD(0)
                         <ul>
                             <li>Loop for a new iterations:</li>
                             <ul>
                                 <li>Sample $(S_t, A_t, R_{t+1}, S_{t+1})$ with replacement from the transitions in the episodes </li>
                                 <li>Update value $V(S_t)$ toward estimated return $\color{red}{R_{t+1}+\gamma V(S_{t+1})}$:</li>
                                 <ul><li>$V(S_t) \leftarrow V(S_t) + \alpha(\color{red}{R_{t+1}+\gamma V(S_{t+1})}-V(S_t))$</li></ul>                             </ul>
                             <li>$R_{t+1}+\gamma V(S_{t+1})$ is called the <i>TD target</i></li>
                             <li>$\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ is called the <i>TD error</i></li>
                         </ul>
                     </li>
                     <li class="substep">If we expand one step further, we got TD(1)
                         <ul>
                             <li>$V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma R_{t+1} + \gamma^2 V(S_{t+2})-V(S_t))$</li>
                             <li>Similarly, we can have TD(2), TD(3), ... </li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Temporal-Difference Learning</h1>
                 <ul>
                     <li>Quick facts:
                         <ul>
                             <li>TD methods learn directly from episodes of experience</li>
                             <li>TD is model-free: no knowledge of MDP transitions / rewards</li>
                             <li>TD learns from incomplete episodes, by <b>bootstrapping</b></li>
                             <li>TD updates a guess towards a guess</li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Key Differences between MC and TD</h1>
                 <ul>
                     <li>MC estimates values based on <i>rollout</i></li>
                     <li>TD estimates values based on Bellman equation</li>
                 </ul>
                 <div class="column">
                     <div class="row">
                         <div class="column" style="flex: 10%">
                             <img src="./L12/mc.png" width="80%"></img>
                         </div>
                         <div class="column" style="flex: 10%">
                             <img src="./L12/td.png" width="80%"></img>
                         </div>
                     </div>
                 </div>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Pros and Cons of MC vs. TD</h1>
                 <img src="./L13/policy.svg" width="50%"/>
                 <ul>
                     <li>TD can learn <i>before</i> knowing the final outcome
                         <ul>
                             <li>MC must wait until the end of episodes</li>
                             <li>TD can learn online after every step</li>
                         </ul>
                     </li>
                 </ul>
             </div>
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Pros and Cons of MC vs. TD</h1>
                 <img src="./L13/policy.svg" width="50%"/>
                 <ul>
                     <li>TD can learn <i>without</i> the final outcome
                         <ul>
                             <li>MC can only learn from complete sequences</li>
                             <li>When some episodes are incomplete, TD can still learn</li>
                             <li>MC only works for episodic (terminating) environments</li>
                             <li>TD works in non-terminating environments</li>
                         </ul>
                     </li>
                 </ul>
             </div>


             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Bias/Variance Trade-Off</h1>
                 <ul>
                     <li>Return $G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-t-1}R_T$ is always an unbiased estimate of $V_\pi(S_t)$</li>
                     <li>The true TD target $R_{t+1}+\gamma V_\pi(S_{t+1})$ is an unbiased estimate of $V_\pi(S_t)$</li>
                     <li>If we will <b>update $\pi$</b> slowly along the learning process,  
                         <ul>
                             <li class="substep">the TD target $R_{t+1}+\gamma V(S_{t+1})$ becomes a <i>biased estimate</i> of $V_\pi(S_t)$, because 
                                 <ul>
                                     <li>$V(S_{t+1})$ is a return estimation from the previous $\pi$. </li>
                                 </ul> 
                             </li>
                             <li class="substep">However, the TD target also has <i>much lower variance</i> than the return $G_t$, because
                                 <ul>
                                     <li>the return $G_t$ is from a single rollout, heavily affected by the randomness (actions, transitions, and rewards) in all the future steps; </li>
                                     <li>whereas the TD target is affected by the randomness in the <i>next one step</i>, and the low variance of the $V(S_{t+1})$ estimation from many historical rollouts.</li>
                                 </ul>
                             </li>
                         </ul>
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Pros and Cons of MC vs TD (2)</h1>
                 <ul>
                     <li>MC has high variance, zero bias
                         <ul>
                             <li>Good convergence properties (even with function approximation)</li>
                             <li>Independent of the initial value of $V$</li>
                             <li>Very simple to understand and use</li>
                         </ul>
                     </li>
                     <li>TD has low variance, some bias
                         <ul>
                             <li>Usually more efficient than MC</li>
                             <li>TD(0) converges to $V_\pi(s)$ (but not always with function approximation)</li>
                             <li>Has certain dependency on the initial value of $V$</li>
                         </ul>
                     </li>
                 </ul>
             </div>
             <!--
                 -[> ################################################################### <]
                 -[> # New section                                                     # <]
                 -[> ################################################################### <]
                 -<div class="step slide separator" id="discussion">
                 -    <h1 class="nt">Discussion</h1>
                 -    <div class="rby">Read by Yourself</div>
                 -</div>
                 -[> ################################################################### <]
                 -<div class="step slide">
                 -    <h1 class="nt">Reinforcement Learning vs. Planning</h1>
                 -    <ul>
                 -        <li>Two fundamental problems in sequential decision making</li>
                 -        <li>Reinforcement Learning:
                 -            <ul>
                 -                <li>The environment is initially unknown</li>
                 -                <li>The agent interacts with the environment</li>
                 -                <li>The agent improves its policy</li>
                 -            </ul>
                 -        </li>
                 -        <li>Planning:
                 -            <ul>
                 -                <li>A model of the environment is known</li>
                 -                <li>The agent performs computations with its model (without any external interaction)</li>
                 -                <li>The agent improves its policy</li>
                 -                <li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>
                 -            </ul>
                 -        </li>
                 -    </ul>
                 -    <div class="rby">Read by Yourself</div>
                 -</div>
                 -[> ################################################################### <]
                 -<div class="step slide">
                 -    <h1 class="nt">Reinforcement Learning vs. Supervised Learning</h1>
                 -    <ul>
                 -        <li>Two fundamental learning paradigms</li>
                 -        <li>Supervised Learning:
                 -            <ul>
                 -                <li>Learning from labels</li>
                 -                <li>Dataset is given</li>
                 -                <li>Data distribution is fixed</li>
                 -                <li>Focus more on generalization</li>
                 -            </ul>
                 -        </li>
                 -        <li>Reinforcement Learning:
                 -            <ul>
                 -                <li>Learning from rewards</li>
                 -                <li>Agent needs to collect dataset by itself</li>
                 -                <li>Data distribution is shifting</li>
                 -                <li>Focus more on optimization</li>
                 -            </ul>
                 -        </li>
                 -    </ul>
                 -    <div class="rby">Read by Yourself</div>
                 -</div>
             -->
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="vt">Bellman, or no Bellman, that is the question</h1>
                 <ul>
                     <li><i>Monte-Carlo sampling</i> and <i>Bellman equation</i> are two fundamental tools of value estimation for policies.</li>
                     <li>Each has its pros and cons. </li>
                     <li>Based on them, there are two families of model-free RL algorithms, both well developed. 
                         Some algorithms leverage both.</li>
                     <li>Fundamentally, it is about the balance between bias and variance (sample complexity).</li>
                 </ul>
             </div>
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="vt">We start from REINFORCE and Deep Q-Learning</h1>
                 <ul>
                     <li>Reason I:
                         <ul>
                             <li>REINFORCE uses Monte-Carlo sampling</li>
                             <li>Deep Q-Learning (DQN) uses Bellman equation</li>
                         </ul>
                     </li>
                     <li>Reason II:</li>
                     <ul>
                         <li>REINFORCE only has a <b>policy network</b> </li>
                         <li>DQN only has a <b>value network</b> </li>
                     </ul>
                 </ul>
             </div>

             <div class="step slide">
                 <h1 class="nt"> Taxonomy of RL Algorithms and Examples</h1>
                 <div class="mermaid" style="text-align:center">
                     graph TD
                     l1("RL Algorithms") 
                     l11("Model-Free RL")
                     l12("Model-Based RL")
                     l111("MC Sampling<br/>")
                     l112("Bellman based<br/>")
                     l121("Learn the Model")
                     l122("Given the Model")
                     l1111("REINFORCE")
                     l1121("Deep Q-Network")
                     l1-->l11 
                     l1-->l12
                     l11-->l111
                     l11-->l112
                     l12-->l121
                     l12-->l122
                     l111-->l1111
                     l112-->l1121
                     style l11 fill:#eadfa4
                     style l111 fill:#eadfa4
                     style l112 fill:#eadfa4
                     style l1111 fill:#eadfa4
                     style l1121 fill:#eadfa4
                 </div>
             </div>
             <!-- ################################################################### -->
             <!-- # New section                                                     # -->
             <!-- ################################################################### -->
             <div class="step slide separator" id="algo">
                 <h1 class="nt">Q-Learning for Tabular RL</h1>
                 <h2><span style="font-weight:normal"><i>Tabular RL: RL with discrete and finite state space, convenient for algorithm development and convergence analysis</i></span></h2>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="et">The Anatomy of an RL algorithm</h1>
                 <div style=margin-top:50px>
                     <img src="./L12/RL_anatomy.png" width="100%"/>
                 </div>
                 <div class="credit"><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf">CS285</a> taught at UC Berkeley by Prof. Sergey Levine.</div>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <!--<h1 class="et">Q-Learning for Tabular RL</h1>-->
                 <div style="margin-top: 100px"></div>
                 <ul>
                     <li>
                         Suppose we are going to learn the Q-function. Let us follow the previous flow chart and answer three questions:
                     </li>
                     <ol>
                         <li>Given transitions $\{(s,a,s',r)\}$ from some trajectories, how to improve the current Q-function?
                             <ul class="substep">
                                 <li>By Temporal Difference learning, the update target for $Q(S,A)$ is
                                     <ul>
                                         <li>$R+\gamma\max_a Q(S', a)$</li>
                                     </ul>
                                 </li>
                                 <li>Take a small step towards the target
                                     <ul>
                                         <li>$Q(S,A)\leftarrow Q(S,A)+\alpha[R+\gamma\max_a Q(S', a)-Q(S,A)]$</li>
                                     </ul>
                                 </li>
                             </ul>
                         </li>
                         <li>Given $Q$, how to improve policy?
                             <ul class="substep">
                                 <li>Take the greedy policy based on the current $Q$
                                     <ul>
                                         <li>$\pi(s)=\text{argmax}_a Q(s,a)$</li>
                                     </ul>
                                 </li>
                             </ul>
                         </li>
                         <li>Given $\pi$, how to generate trajectories?
                             <ul class="substep">
                                 <li>Simply run the greedy policy in the environment. </li>
                                 <li>Any issues? </li>
                             </ul>
                         </li>
                     </ol>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Failure Example</h1>
                 <div class="row">
                     <div class="column">
                         <ul>
                             <li>Initialize Q
                                 <ul>
                                     <li>$Q(s_0,a_1)=0,Q(s_0,a_2)=0$</li>
                                     <li>$\pi(s_0)=a_1$</li>
                                 </ul>
                             </li>
                             <li>Iteration 1: take $a_1$ and update $Q$
                                 <ul>
                                     <li>$Q(s_0,a_1)=1,Q(s_0,a_2)=0$</li>
                                     <li>$\pi(s_0)=a_1$</li>
                                 </ul>
                             </li>
                             <li>Iteration 2: take $a_1$ and update $Q$
                                 <ul>
                                     <li>$Q(s_0,a_1)=1,Q(s_0,a_2)=0$</li>
                                     <li>$\pi(s_0)=a_1$</li>
                                 </ul>
                             </li>
                             <li>...</li>
                             <li><b>$Q$ stops to improve because the agent is too greedy!</b></li>
                         </ul>
                     </div>
                     <div class="column" style="flex:30%">
                         <div style=margin-top:10px>
                             <img src="./L12/eps_greedy_mdp.png" width="70%"/>
                         </div>
                     </div>
                 </div>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">$\epsilon$-Greedy Exploration</h1>
                 <ul>
                     <li>The simplest and most effective idea for ensuring continual exploration</li>
                     <li>With probability $1-\epsilon$ choose the greedy action</li>
                     <li>With probability $\epsilon$ choose an action at random</li>
                     <li>All $m$ actions should be tried with non-zero probability</li>
                     <li>Formally,
                         \[
                         \pi_*(a|s)=
                         \begin{cases}
                         \epsilon/m + 1-\epsilon, & \text{if}~~a=\text{argmax}_{a\in\mc{A}}~Q(s,a) \\  
                         \epsilon/m, & \text{otherwise}   
                         \end{cases}
                         \]
                     </li>
                 </ul>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="nt">Exploration vs Exploitation</h1>
                 <ul>
                     <!--
                         -<li>Two fundamental behaviours of RL agents
                         -    <ul>
                         -        <li>Reinforcement learning is like trial-and-error learning</li>
                         -        <li>The agent should discover a good policy</li>
                         -        <li>From its experiences of the environment</li>
                         -        <li>Without losing too much reward along the way</li>
                         -    </ul>
                         -</li>
                     -->
                     <li>Exploration
                         <ul>
                             <li>finds more information about the environment</li>
                             <li>may waste some time</li>
                         </ul>
                     </li>
                     <li>Exploitation
                         <ul>
                             <li>exploits known information to maximize reward</li>
                             <li>may miss potential better policy</li>
                         </ul>
                     </li>
                     <li>Balancing exploration and exploitation is a key problem of RL. We use will spend a lecture to discuss advanced exploration strategies.</li>
                 </ul>
             </div>
             <!-- ################################################################### -->
             <div class="step slide">
                 <h1 class="et">Q-Learning Algorithm</h1>
                 <div style=margin-top:100px>
                     <img src="./L12/q_learning_code.png" width="100%"/>
                 </div>
             </div>

             <!-- ################################################################### -->
             <div class="step slide">
                 <!-- <ul>
                     <li>Iteratively updating Q function to convergence, we get</li>
                     </ul> -->
                     <img src="./L12/maze_q_value.png" width="55%"/>
                     <p>Running Q-learning on Maze</p>
                     <div class="credit">Plot with the tools in <a href="https://drive.google.com/file/d/177mrb9B4rqNrdTLtZSgPRdCnrhVlJdEx/view">an awesome playground for value-based RL</a> from Justin Fu</div>
             </div>
             
             <!-- ################################################################### -->
             <div class="step slide">
                 <div style="margin-top:300px"></div>
                 <center>
                     <div class="Large"><b>End</b></div>
                 </center>
             </div>

        </div>

        <!--
            Add navigation-ui controls: back, forward and a select list.
            Add a progress indicator bar (current step / all steps)
            Add the help popup plugin
        -->
        <div id="impress-toolbar"></div>

        <div class="impress-progressbar"><div></div></div>
        <div class="impress-progress"></div>

        <div id="impress-help"></div>

        <script type="text/javascript" src="../extras/highlight/highlight.pack.js"></script>
        <script src="../extras/mermaid/mermaid.min.js"></script>
        <script type="text/javascript" src="../extras/markdown/markdown.js"></script>
        <!--
            To make all described above really work, you need to include impress.js in the page.
            You also need to call a `impress().init()` function to initialize impress.js presentation.
            And you should do it in the end of your document. 
        -->
        <script>
            function setSlideID() {
                x = document.getElementsByClassName("slide");
                const titleSet = new Set();
                var titleDict = {};
                for (var i = 2; i < x.length; i++) {
                    h1 = x[i].getElementsByTagName("h1")[0];
                    if (h1) {
                        // alert(title);
                        title = '--'+h1.innerHTML.replace(/\W/g, '');
                        if (titleSet.has(title)) {
                            titleDict[title] += 1;
                            title = title + '_' + titleDict[title].toString();
                        }
                        else {
                            titleSet.add(title);
                            titleDict[title] = 1;
                        }
                        x[i].id = title;
                    }
                }
            }
            setSlideID(); 
        </script>
        <script>
            function getTitles() {
                var secs = document.getElementsByClassName("separator");
                var titleList = [];
                var titleIdList = [];
                const titleIdSet = new Set();
                for (var i = 0; i < secs.length; i++) {
                    h1 = secs[i].getElementsByTagName("h1")[0];
                    titleId = 'Sec:'+h1.innerHTML.replace(/\W/g, '');
                    if (titleIdSet.has(titleId)) {
                        continue;
                    }
                    titleIdSet.add(titleId);
                    titleList.push(h1.innerHTML);
                    titleIdList.push(titleId);
                    secs[i].id = titleId;
                }
                console.log(titleList);
                return [titleList, titleIdList];
            }

            function addToC(titleList, titleIdList){
                var agenda = document.getElementById("agenda");
                agenda.innerHTML = '';
                for (var i = 0; i < titleList.length; i++) {
                    agenda.innerHTML += '<li><a href="#'+titleIdList[i]+'">'+titleList[i]+'</a></li>';
                }
            }

            res = getTitles();
            titleList = res[0]; titleIdList  = res[1];
            addToC(titleList, titleIdList);
        </script>
        <script type="text/javascript" src="../js/impress.js"></script>
        <script type="text/javascript">
            (function(){
                var vizPrefix = "language-viz-";
                Array.prototype.forEach.call(document.querySelectorAll("[class^=" + vizPrefix + "]"), function(x){
                    var engine;
                    x.getAttribute("class").split(" ").forEach(function(cls){
                        if (cls.startsWith(vizPrefix)) {
                            engine = cls.substr(vizPrefix.length);
                        }
                    });
                    var image = new DOMParser().parseFromString(Viz(x.innerText, {format:"svg", engine:engine}), "image/svg+xml");
                    x.parentNode.insertBefore(image.documentElement, x);
                    x.style.display = 'none'
                    x.parentNode.style.backgroundColor = "white"
                });
            })();
            window.MathJax = {
                tex2jax: {
                    inlineMath: [['$','$'], ['\\(','\\)']],
                    displayMath: [['$$','$$'], ['\\[','\\]']],
                    processEscapes: true,
                    processEnvironments: true,
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                    TeX: { equationNumbers: { autoNumber: "AMS" },
                        extensions: ["AMSmath.js", "AMSsymbols.js", "color.js"],
                    },
                    jax: ["input/TeX", "output/SVG"]
                },
                AuthorInit: function () {
                    MathJax.Hub.Register.StartupHook("Begin",function () {
                        MathJax.Hub.Queue(function() {
                            var all = MathJax.Hub.getAllJax(), i;
                            for(i = 0; i < all.length; i += 1) {
                                all[i].SourceElement().parentNode.className += ' has-jax';
                            }
                        })
                    });
                }
            };
        </script>
        <script type="text/javascript" src="../extras/mathjax/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script>impress().init();</script>
    </body>
</html>
<!-- discarded -->
